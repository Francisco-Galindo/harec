# Compiler

<div align="center">
  <img src="https://cloudfront-us-east-1.images.arcpublishing.com/infobae/QLXAPU64VVD7DMR5ZF7VIEH4HQ.jpg" alt="Logo UNAM" width="200"/>
  <p> Universidad Nacional Autónoma de México </p>
  <p> Ingeniería en Computación </p>
  <p> Compiladores </p>
  <p> Lexer - Lexical Analysis </p>
  <p> Alumnos: </p>
  <p>320198388</p>
  <p>320051665</p>
  <p>320298608</p>
  <p>320244612</p>
  <p>320054336</p>
  <p> Grupo 5 </p>
  <p> Semestre 2025-2 </p>
  <p> México, CDMX. Marzo 2025 </p>
</div>


## Introduction
In this project, we will develop a compiler and its corresponding parser using the Hare programming language, both as the implementation language and as the language to be compiled. The main focus will be on the design and implementation of the syntax and semantic analysis stages, which are essential components of any compiler.

Syntax analysis verifies the structural correctness of the source code according to the language's grammar. This involves constructing a parser capable of identifying valid program constructs and producing an intermediate representation, typically in the form of a syntax tree. For this stage, we develop a recursive descent parser by applying theoretical concepts.

Semantic analysis complements syntax analysis by checking that the code adheres to the language's rules beyond its form—ensuring, for example, that operations are applied to compatible types and that identifiers are properly declared. It also involves building a symbol table to store relevant information about program elements.

Creating a compiler is necessary for understanding how programming languages are processed and executed. By completing this project, we aim to produce a working compiler for Hare and gain practical knowledge of compiler construction techniques, particularly those related to parsing and semantic validation.

## Theorical Background
### What is a Compiler?
A compiler is a program that reads a program written in one language, the source language, and translates it into an equivalent program in another language, the target language. In essence, a compiler maps a source program into a semantically equivalent target program. This mapping process consists of two main parts: analysis and synthesis.

The analysis phase breaks the source program into its constituent components and assigns a grammatical structure to them. Based on this structure, it generates an intermediate representation of the source program. During this phase, the compiler also collects information about the program and stores it in a data structure known as the symbol table. This table, along with the intermediate representation, is passed to the synthesis phase.

The synthesis phase uses this intermediate representation and the symbol table to construct the corresponding target program. Compilation is typically organized as a sequence of phases, with each phase transforming the program representation in preparation for the next.

The first phase of a compiler is called lexical analysis or scanning. The lexical analyzer reads the stream of characters from the source program and groups them into meaningful sequences called lexemes. For each lexeme, it produces a corresponding token, which is then passed to the next phase, syntax analysis.

The second phase, known as syntax analysis or parsing, uses the tokens generated by the lexical analyzer to build a tree-like intermediate structure that reflects the grammatical organization of the token stream. A common form of this structure is the syntax tree, where each internal node represents an operation, and the children represent its operands. The syntax tree visually illustrates the order in which operations are to be executed.

The semantic analyzer takes the syntax tree and symbol table to ensure the source program complies with the language's semantic rules. It also gathers type information, storing it in either the syntax tree or the symbol table for later use during intermediate code generation. A key task during semantic analysis is type checking, where the compiler verifies that each operator is used with correctly typed operands.

As the compiler translates the source program into target code, it may generate one or more intermediate representations, which can take various forms. After completing syntax and semantic analysis, many compilers produce a low-level, machine-like intermediate representation, essentially a program for an abstract machine. This is often expressed as a sequence of three-address code instructions.

The code generation phase takes this intermediate representation and translates it into the target language. If the target is machine code, the compiler allocates registers or memory locations for each variable and translates the intermediate instructions into machine instructions that perform the equivalent tasks.

An important function of a compiler is to record the variable names used in the source program and track information about each one’s attributes. This is managed through the symbol table, a data structure that contains an entry for each variable, with fields for its associated attributes.

For large programs, compilation is often done in parts. The resulting relocatable machine code may need to be linked with other object files and libraries to form a complete executable. The linker resolves references to external memory addresses where code in one file refers to locations in another. Finally, the loader places the fully linked executable into memory, ready for execution.

### About the sintax analysis
Parsing is a fundamental process in the field of computer science, especially within the domains of compilers, interpreters, and language processing systems. It involves analyzing a sequence of tokens or symbols based on the rules defined by a formal grammar, with the purpose of constructing a syntactic structure, often in the form of a parse tree or abstract syntax tree. Parsing techniques are generally classified into top-down and bottom-up methods. Recursive descent parsing belongs to the former category and is one of the most widely understood and implemented parsing strategies due to its straightforward and modular approach.

Recursive descent parsing operates by associating each non-terminal in a grammar with a corresponding function in the parser. These functions call one another in a recursive manner to recognize structures in the input, hence the term "recursive descent". As the input string is processed from left to right, the parser attempts to apply production rules that correspond to the structure of the grammar, descending through the hierarchy of rules until it either accepts the input or fails due to a mismatch.

This method is particularly suitable for grammars that conform to the LL(1) class, meaning they can be parsed from left to right with Leftmost derivation using one lookahead token. However, for recursive descent to function correctly, the grammar must be free of left recursion. Left-recursive productions, which allow a non-terminal to appear as the leftmost symbol in one of its own derivations, can lead to infinite recursion and must therefore be transformed before the grammar can be parsed using this technique. Similarly, ambiguous or poorly factored grammars may require rewriting to ensure that decisions can be made deterministically based on the next token in the input stream.

Despite its advantages, recursive descent parsing does have limitations. Its reliance on grammars that are LL(1) restricts its applicability to a subset of possible languages. Moreover, maintaining a recursive descent parser for a large and complex grammar can become cumbersome and error-prone, particularly when compared to automated parser generators or more powerful bottom-up parsing techniques. Nevertheless, for many use cases, particularly those involving smaller grammars or where control and transparency are valued over generality, recursive descent remains an effective and reliable method.

### About the semantic analysis


### Compiler

## Development
### Parser
For the parser there are multiple files used. With type.ha, using the lexer we obtain the datatypes and tokens of the elements so as to know what kind of expression are we working with.
with type.ha, we define the abstract type of data to represent expressions, verifying if the document is woring with normal expressions, with control structures, etc. After verifying all of this, it generates an abstract sintax tree depending on the type of structure found. The parser itself is in parse.ha which takes raw tokens produced by the lexer and transforms them into structured representations (AST nodes) that the compiler can analyze and eventually compile.

The primary function exposed from this file is called decls. It receives a pointer to a lexer and returns a list of parsed declarations. Internally, it uses a loop to keep parsing declarations until the lexer indicates that it has reached the end of the input. For each declaration it finds, it calls a helper function named decl_func, appends the resulting AST node to a list, and ensures that every declaration is followed by a semicolon. This function essentially builds up a complete list of top-level function declarations present in a source file.

The real core of the parser logic lies in the decl_func function. This function begins by confirming that the current token is the keyword fn, signaling the start of a function declaration. Then it expects an identifier, which will become the function’s name. After that, it parses the function’s prototype, which includes both the parameter list and the return type. This is stored using a standard AST type wrapper, and the location of the prototype in the source code is also recorded.

After parsing the prototype, the parser checks whether the function has a body or is merely declared without a body. This is done by looking for either an equal sign or a semicolon. If it finds an equal sign, the function has a body, and the parser expects a valid expression to follow, which it parses and stores as the function’s body. If it finds a semicolon, the parser assumes the function is only being declared and does not define a body at this point.

The resulting data is wrapped in an AST node representing a function declaration. This includes the function's name, prototype, body (if any), and some metadata like source code locations and attributes. The parser sets default values for things like whether the function is exported or whether it has any documentation, though those features could be added in the future.

### Compiler construction



### Test inputs



## Results



## Conclusion
This project demonstrates the practical application of theoretical concepts in compiler construction, highlighting the structured progression from abstract syntax rules to concrete code generation. By implementing a compiler and parser for the Hare programming language, written in Hare itself, we reinforced foundational principles such as formal grammars, lexical analysis, syntax analysis, and semantic analysis.

Instead of relying on external libraries during the syntax analysis phase, we chose to build a recursive descent parser based on theoretical concepts covered in class and supported by bibliographic references. The semantic analysis phase was implemented through a symbol table that validates identifiers by checking their presence within a defined scope. This approach aligns with the theoretical model of symbol resolution and scope management, illustrating how simple data structures can effectively enforce semantic rules.

Furthermore, by targeting the RISC-V architecture for code generation, the project required precise translation from high-level constructs to a register-based, low-level instruction set. This provided a concrete application of the theoretical mapping between intermediate representations and target machine code.

Our compiler, although basic in comparison to a Java compiler, supports block-structured code such as if statements, for loops, and functions, allowing simple programs to be parsed, compiled, and executed successfully.

Overall, the project exemplifies how theoretical knowledge, such as grammar definitions, parsing strategies, scope rules, and target architecture modeling, can be systematically applied to develop a working compiler. The results validate the importance of a solid theoretical foundation for solving complex systems-level programming problems.
## Sources

